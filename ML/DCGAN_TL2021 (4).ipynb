{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DCGAN_TL2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4kNYbeVfELHx",
        "MYleZS-UFHe1",
        "QPq1Dn9yFB6b",
        "z7XLN-h_FyQf",
        "7XgaozbBF1Es",
        "s1_SNNvnF_Md",
        "Eob3OpIqGCsP",
        "6-w1xbvPGGra",
        "44v4Wlm2GOiT",
        "39D5AQgjGP-P",
        "o1cTaVYB5G6L",
        "_Wk-S07yGVQs",
        "AHMcqY_NGZfg",
        "rMGoO7nlGcSg",
        "CZuGVb8UGef-",
        "EiLFwdv6dPjz",
        "N97o_bXfG5hV",
        "i3sawVObGyvh",
        "pAubVjLCHF7v",
        "iTaZ9ZXWHu64",
        "MMLg0UN7IB2_"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4597e93b08e64d899380e68f6d1420a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fe6f4ac069514770941f8833e92dd4c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a5f5e84d21ca40868e40ac7c42b31f77",
              "IPY_MODEL_099374e302c6494d88830736fd0a104a"
            ]
          }
        },
        "fe6f4ac069514770941f8833e92dd4c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5f5e84d21ca40868e40ac7c42b31f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_246c19413a8248efa21c2267c4493dfe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.18MB of 2.18MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea047b5de5b94db4a40d73e05ab1d7f2"
          }
        },
        "099374e302c6494d88830736fd0a104a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cfba2fde04024303886c44d5c6503753",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03bf12ce12e24cbf9d4fb1e49cc41d8e"
          }
        },
        "246c19413a8248efa21c2267c4493dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea047b5de5b94db4a40d73e05ab1d7f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cfba2fde04024303886c44d5c6503753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03bf12ce12e24cbf9d4fb1e49cc41d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kNYbeVfELHx"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "s76sDKr-EN9r",
        "outputId": "37f51428-2b9d-4073-a090-e42c3bc9eae5"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload your kaggle.json here.\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Kaggle datasets here.\n",
        "\n",
        "# Animal Faces.\n",
        "!mkdir animal-faces\n",
        "%cd animal-faces/\n",
        "!kaggle datasets download -d andrewmvd/animal-faces\n",
        "!unzip -q animal-faces.zip\n",
        "!rm animal-faces.zip\n",
        "%cd ../\n",
        "\n",
        "# Cat Faces by fferlito.\n",
        "!mkdir fferlito-cat-faces\n",
        "%cd fferlito-cat-faces/\n",
        "!kaggle datasets download -d vincenttu/catfacesdatasetfferlito\n",
        "!unzip -q catfacesdatasetfferlito.zip\n",
        "!rm catfacesdatasetfferlito.zip\n",
        "%cd ../\n",
        "\n",
        "import tarfile\n",
        "\n",
        "tgz = tarfile.open(r\"/content/fferlito-cat-faces/dataset-part1.tar\")\n",
        "tgz.extractall(path=\"/content/fferlito-cat-faces\")\n",
        "tgz.close()\n",
        "tgz = tarfile.open(r\"/content/fferlito-cat-faces/dataset-part2.tar\")\n",
        "tgz.extractall(path=\"/content/fferlito-cat-faces\")\n",
        "tgz.close()\n",
        "tgz = tarfile.open(r\"/content/fferlito-cat-faces/dataset-part3.tar\")\n",
        "tgz.extractall(path=\"/content/fferlito-cat-faces\")\n",
        "tgz.close()\n",
        "\n",
        "!rm fferlito-cat-faces/dataset-part1.tar\n",
        "!rm fferlito-cat-faces/dataset-part2.tar\n",
        "!rm fferlito-cat-faces/dataset-part3.tar\n",
        "\n",
        "# Cat Faces by Spandan.\n",
        "!mkdir spandan-cat-faces\n",
        "%cd spandan-cat-faces/\n",
        "!kaggle datasets download -d spandan2/cats-faces-64x64-for-generative-models\n",
        "!unzip -q cats-faces-64x64-for-generative-models.zip\n",
        "!rm cats-faces-64x64-for-generative-models.zip\n",
        "%cd ../\n",
        "\n",
        "# Cat Faces by waifuai.\n",
        "!mkdir waifuai-cat-faces\n",
        "%cd waifuai-cat-faces/\n",
        "!kaggle datasets download -d waifuai/cat2dog\n",
        "!unzip -q cat2dog.zip\n",
        "!rm cat2dog.zip\n",
        "%cd ../\n",
        "\n",
        "# CelebA.\n",
        "!mkdir celeba\n",
        "%cd celeba/\n",
        "!kaggle datasets download -d zuozhaorui/celeba\n",
        "!unzip -q celeba.zip\n",
        "!rm celeba.zip\n",
        "%cd ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-842aa1c5-d7b9-4994-8c4b-2d92ba316a7d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-842aa1c5-d7b9-4994-8c4b-2d92ba316a7d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr2q1SojEUQw"
      },
      "source": [
        "!pip install albumentations --upgrade -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJdUpNr5EXjK"
      },
      "source": [
        "from __future__ import print_function\n",
        "from IPython.display import HTML\n",
        "import argparse\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import imageio\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "#%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RTSu_Ka7nG9"
      },
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "# from wandb.keras import WandbCallback\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYleZS-UFHe1"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint Directory Function"
      ],
      "metadata": {
        "id": "uOVSnySrOvoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For checkpointing.\n",
        "def return_ckpt_dir(GAN_player):\n",
        "  return '{}/{}_{}.pth'.format(c.ckpt_dir, c.model_name, GAN_player)  # We are following this name convention."
      ],
      "metadata": {
        "id": "njj2vsw8OvNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seed Function"
      ],
      "metadata": {
        "id": "Uw1HUFzB-YOy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Wgb3_cFG9W"
      },
      "source": [
        "def seed_everything(seed=999):\n",
        "  # Set random seed for reproducibility.\n",
        "  print(\"Seeding everything...\")\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight Init Function\n",
        "\n",
        "(```mean = 0```, ```stdev = 0.02```)"
      ],
      "metadata": {
        "id": "1f4pW6Y3-VS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        try:\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "            nn.init.constant_(m.bias.data, 0)\n",
        "        except:\n",
        "            print(\"Unable to initialize Conv layer weights.\")\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        try:\n",
        "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            nn.init.constant_(m.bias.data, 0)\n",
        "        except:\n",
        "          print(\"Unable to initialize BatchNorm weights.\")"
      ],
      "metadata": {
        "id": "x6JkNj4i-PIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPq1Dn9yFB6b"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgpL_f887p4u",
        "outputId": "8dba8282-f4eb-4e18-f87c-8b6d33757f52"
      },
      "source": [
        "class Config:\n",
        "\n",
        "    # Project name.\n",
        "    project_name = \"DCGAN_TL2021\"\n",
        "\n",
        "    # Project run name.\n",
        "    project_run_name = \"CycleGAN\"\n",
        "\n",
        "    # Model name.\n",
        "    model_name = \"CycleGAN\"\n",
        "\n",
        "    # Checkpoint dir.\n",
        "    ckpt_dir = \".\"\n",
        "\n",
        "    # Number of workers for dataloader.\n",
        "    workers = 0\n",
        "\n",
        "    # How many images out of CelebA dataset to select from.\n",
        "    total_images = 1000\n",
        "\n",
        "    # How many images to select out of total_images.\n",
        "    num_images = 64\n",
        "\n",
        "    # Number of GPUs available. Use 0 for CPU mode..\n",
        "    ngpu = 1\n",
        "\n",
        "    # Choosing the device.\n",
        "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "    seed_everything()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seeding everything...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZnucdDXFW2R"
      },
      "source": [
        "c = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7XLN-h_FyQf"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat_path = r\"/content/animal-faces/afhq/train/cat\"  # path_c\n",
        "train_dog_path = r\"/content/animal-faces/afhq/train/dog\"\n",
        "train_wild_path = r\"/content/animal-faces/afhq/train/wild\"\n",
        "\n",
        "val_cat_path = r\"/content/animal-faces/afhq/val/cat\"  # path_c_1\n",
        "val_dog_path = r\"/content/animal-faces/afhq/val/dog\"\n",
        "val_wild_path = r\"/content/animal-faces/afhq/val/wild\"\n",
        "\n",
        "celeba_path = r\"/content/celeba/img_align_celeba/img_align_celeba\"\n",
        "\n",
        "fferlito_path = list(glob(\"/content/fferlito-cat-faces/*\"))  # path_c_fferlito\n",
        "spandan_path = r\"/content/spandan-cat-faces/cats/\"  # path_c_spandan\n",
        "waifuai_path = [os.path.join(r\"/content/waifuai-cat-faces/cat2dog\", p) for p in [\"trainA\", \"testA\"]]  # path_c_waifuai"
      ],
      "metadata": {
        "id": "VD6QOQ6lEyGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Animal-Faces Cat & CelebA Preprocessing"
      ],
      "metadata": {
        "id": "DXCIIBSSPIVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AFCatCelebADataset(Dataset):\n",
        "    def __init__(self, path_c, path_h, image_size, \n",
        "                 path_c_1=None, path_c_fferlito=None, path_c_spandan=None, path_c_waifuai=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.path_c = list(glob(os.path.join(path_c, \"*\")))\n",
        "\n",
        "        # Adding extra cat images from the val folder.\n",
        "        if path_c_1:\n",
        "            self.path_c.extend(list(glob(os.path.join(path_c_1, \"*\"))))\n",
        "          \n",
        "        # Adding extra cat images from fferlito repo.\n",
        "        if path_c_fferlito:\n",
        "            assert isinstance(path_c_fferlito, list), \"path_c_fferlito should be a list.\"\n",
        "            assert len(path_c_fferlito) == 3, \"path_c_fferlito should be of length 3.\"\n",
        "            for path in path_c_fferlito:\n",
        "                self.path_c.extend(list(glob(os.path.join(path, \"*\"))))\n",
        "\n",
        "        # Adding extra cat images from spandan dataset.\n",
        "        if path_c_spandan:\n",
        "            self.path_c.extend(list(glob(os.path.join(spandan_path, \"*.jpg\"))))  # cats folder is then removed.\n",
        "            \n",
        "\n",
        "      # Adding extra cat images from waifuai dataset.\n",
        "        if path_c_waifuai:\n",
        "            assert isinstance(path_c_waifuai, list), \"path_c_waifuai should be a list.\"\n",
        "            assert len(path_c_waifuai) == 2, \"path_c_waifuai should be of length 2.\"\n",
        "            for path in path_c_waifuai:\n",
        "                self.path_c.extend(list(glob(os.path.join(path, \"*\"))))\n",
        "\n",
        "        self.path_h = list(glob(os.path.join(path_h, \"*\")))\n",
        "\n",
        "        # Method 1 for preparing the data.\n",
        "        self.length_dataset = max(len(self.path_c), len(self.path_h)) # 5153, 203k\n",
        "\n",
        "        self.c_length = len(self.path_c)\n",
        "        self.h_length = len(self.path_h)\n",
        "\n",
        "        self.image_size = image_size  # [H, W]\n",
        "\n",
        "        self.transform = A.Compose([\n",
        "          A.Resize(self.image_size[0], self.image_size[1]),\n",
        "          A.CenterCrop(self.image_size[0], self.image_size[1]),\n",
        "\n",
        "          A.HorizontalFlip(p=0.5),\n",
        "\n",
        "          A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "          ToTensorV2(),\n",
        "        ], additional_targets={\"image0\": \"image\"},)\n",
        "\n",
        "        # Method 2 for preparing the data.\n",
        "        self.path = list(zip(self.path_c, self.path_h))  # Goes till the shortest list ends; a list of tuples of length 5153.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "        # return self.length_dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Method 1 for preparing the data.\n",
        "        # cat_path = self.path_c[index % self.c_length]\n",
        "        # human_path = self.path_h[index % self.h_length]\n",
        "        \n",
        "        # cat_img = np.asarray(Image.open(cat_path))\n",
        "        # human_img = np.asarray(Image.open(human_path))\n",
        "\n",
        "        # aug_imgs = self.transform(image=cat_img, image0=human_img)\n",
        "        # cat_img = aug_imgs[\"image\"]\n",
        "        # human_img = aug_imgs[\"image0\"]\n",
        "\n",
        "        # return {\"cat_img\": cat_img,\n",
        "        #         \"human_img\": human_img}\n",
        "\n",
        "        # Method 2 for preparing the data.\n",
        "        cat_path, human_path = self.path[index]\n",
        "        cat_img = np.asarray(Image.open(cat_path))\n",
        "        human_img = np.asarray(Image.open(human_path))\n",
        "        aug_imgs = self.transform(image=cat_img, image0=human_img)\n",
        "        cat_img = aug_imgs[\"image\"]\n",
        "        human_img = aug_imgs[\"image0\"]\n",
        "\n",
        "        return {\"cat_img\": cat_img,\n",
        "                \"human_img\": human_img}"
      ],
      "metadata": {
        "id": "hDPtyoTAPH6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1cTaVYB5G6L"
      },
      "source": [
        "## Fixed CelebA Image Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sy-BDMI5Kme"
      },
      "source": [
        "class FixedCelebADataset(Dataset):\n",
        "    def __init__(self, path, image_size, num_images, total_images):\n",
        "        super().__init__()\n",
        "\n",
        "        path = list(glob(os.path.join(path, \"*\")))\n",
        "        assert total_images <= len(path), \"total_images should be <= len(path).\"\n",
        "        assert num_images <= total_images, \"num_images should be <= total_images.\"\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.num_images = num_images\n",
        "        self.total_images = total_images\n",
        "        rnd_choices = np.random.choice(total_images, num_images, replace=False)\n",
        "        self.path = np.array(path)[rnd_choices]\n",
        "\n",
        "        self.transform = A.Compose([\n",
        "            A.Resize(self.image_size[0], self.image_size[1]),\n",
        "            A.CenterCrop(self.image_size[0], self.image_size[1]),\n",
        "            A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "          img = np.asarray(Image.open(self.path[index]))\n",
        "          img = self.transform(image=img)[\"image\"]\n",
        "\n",
        "          return {\"images\": img,\n",
        "                  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wk-S07yGVQs"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMGoO7nlGcSg"
      },
      "source": [
        "## UNet-DCGAN\n",
        "\n",
        "Ref: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7GQeez6Gc0X"
      },
      "source": [
        "class DCGANGenerator(nn.Module):\n",
        "    def __init__(self, nc, ngf):\n",
        "        super().__init__()\n",
        "\n",
        "        self.unet_left_block1 = self._unet_left_block(nc, ngf, 4, 2, 1)\n",
        "        self.unet_left_block2 = self._unet_left_block(ngf, ngf * 2, 4, 2, 1)\n",
        "        self.unet_left_block3 = self._unet_left_block(ngf * 2, ngf * 4, 4, 2, 1)\n",
        "        self.unet_left_block4 = self._unet_left_block(ngf * 4, ngf * 8, 4, 2, 1)\n",
        "        self.unet_left_block5 = self._unet_left_block(ngf * 8, nz, 4, 1, 0)        \n",
        "\n",
        "        self.unet_right_block1 = self._unet_right_block(nz, ngf * 8, 4, 1, 0)\n",
        "        self.unet_right_block2 = self._unet_right_block(ngf * 8, ngf * 4, 4, 2, 1)\n",
        "        self.unet_right_block3 = self._unet_right_block(ngf * 4, ngf * 2, 4, 2, 1)\n",
        "        self.unet_right_block4 = self._unet_right_block(ngf * 2, ngf, 4, 2, 1)\n",
        "        self.unet_right_block5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def _unet_right_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def _unet_left_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        # B: 128\n",
        "        # nz: 100\n",
        "        # ngf: 64\n",
        "        # nc: 3\n",
        "\n",
        "        # input: (B x (nc) x 64 x 64)\n",
        "\n",
        "        x = input\n",
        "\n",
        "        # Left block.\n",
        "        x1 = self.unet_left_block1(x)  # (B x (ngf) x 32 x 32)\n",
        "        x2 = self.unet_left_block2(x1)  # (B x (ngf*2) x 16 x 16)\n",
        "        x3 = self.unet_left_block3(x2)  # (B x (ngf*4) x 8 x 8)\n",
        "        x4 = self.unet_left_block4(x3)  # (B x (ngf*8) x 4 x 4)\n",
        "        x5 = self.unet_left_block5(x4)  # (B x nz x 1 x 1)\n",
        "\n",
        "        # Right block.\n",
        "        z1 = self.unet_right_block1(x5)  # (B x (ngf*8) x 4 x 4)\n",
        "        z1 = z1 + x4\n",
        "        z2 = self.unet_right_block2(z1)  # (B x (ngf*4) x 8 x 8)\n",
        "        z2 = z2 + x3\n",
        "        z3 = self.unet_right_block3(z2)  # (B x (ngf*2) x 16 x 16)\n",
        "        z3 = z3 + x2\n",
        "        z4 = self.unet_right_block4(z3)  # (B x (ngf) x 32 x 32)\n",
        "        z4 = z4 + x1\n",
        "        z5 = self.unet_right_block5(z4)  # (B x (nc) x 64 x 64)\n",
        "        z5 = z5 + x\n",
        "\n",
        "        return z5  # (B x (nc) x 64 x 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGANDiscriminator(nn.Module):\n",
        "    def __init__(self, nc, ndf):\n",
        "        super().__init__()\n",
        "\n",
        "        # B: 128\n",
        "        # nc: 3\n",
        "        # ndf: 64\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # input: (B x (nc) x 64 x 64)\n",
        "\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),  # B x (ndf) x 32 x 32\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            self._block(ndf, ndf * 2, 4, 2, 1),  # B x (ndf*2) x 16 x 16\n",
        "            self._block(ndf * 2, ndf * 4, 4, 2, 1),  # B x (ndf*4) x 8 x 8\n",
        "            self._block(ndf * 4, ndf * 8, 4, 2, 1),  # B x (ndf*8) x 4 x 4\n",
        "\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),  # B x 1 x 1 x 1\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input: (B x (nc) x 64 x 64)\n",
        "\n",
        "        return self.main(input)  # B x 1 x 1 x 1"
      ],
      "metadata": {
        "id": "otyJoRcm0N1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiLFwdv6dPjz"
      },
      "source": [
        "## PatchGAN \n",
        "Ref: https://github.com/znxlwm/pytorch-pix2pix/blob/3059f2af53324e77089bbcfc31279f01a38c40b8/network.py#L104"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkJYYgbddPD_"
      },
      "source": [
        "class PatchGANGenerator(nn.Module):\n",
        "    def __init__(self, d=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # Unet Encoder\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(d * 2)\n",
        "        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(d * 4)\n",
        "        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 2, 1)\n",
        "        self.conv4_bn = nn.BatchNorm2d(d * 8)\n",
        "        self.conv5 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n",
        "        self.conv5_bn = nn.BatchNorm2d(d * 8)\n",
        "        self.conv6 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n",
        "        self.conv6_bn = nn.BatchNorm2d(d * 8)\n",
        "\n",
        "        # self.conv7 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n",
        "        # self.conv7_bn = nn.BatchNorm2d(d * 8)\n",
        "        # self.conv8 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n",
        "        # # self.conv8_bn = nn.BatchNorm2d(d * 8)\n",
        "\n",
        "        # UNet Decoder\n",
        "\n",
        "        # self.deconv1 = nn.ConvTranspose2d(d * 8, d * 8, 4, 2, 1)\n",
        "        # self.deconv1_bn = nn.BatchNorm2d(d * 8)\n",
        "        # self.deconv2 = nn.ConvTranspose2d(d * 8 * 2, d * 8, 4, 2, 1)\n",
        "        # self.deconv2_bn = nn.BatchNorm2d(d * 8)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(d * 8, d * 8, 4, 2, 1)\n",
        "        self.deconv3_bn = nn.BatchNorm2d(d * 8)\n",
        "        self.deconv4 = nn.ConvTranspose2d(d * 8 * 2, d * 8, 4, 2, 1)\n",
        "        self.deconv4_bn = nn.BatchNorm2d(d * 8)\n",
        "        self.deconv5 = nn.ConvTranspose2d(d * 8 * 2, d * 4, 4, 2, 1)\n",
        "        self.deconv5_bn = nn.BatchNorm2d(d * 4)\n",
        "        self.deconv6 = nn.ConvTranspose2d(d * 4 * 2, d * 2, 4, 2, 1)\n",
        "        self.deconv6_bn = nn.BatchNorm2d(d * 2)\n",
        "        self.deconv7 = nn.ConvTranspose2d(d * 2 * 2, d, 4, 2, 1)\n",
        "        self.deconv7_bn = nn.BatchNorm2d(d)\n",
        "        self.deconv8 = nn.ConvTranspose2d(d * 2, 3, 4, 2, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input: (B, 3, 64, 64)\n",
        "\n",
        "        e1 = self.conv1(input)  # (B, 64, 32, 32)\n",
        "        e2 = self.conv2_bn(self.conv2(F.leaky_relu(e1, 0.2)))  # (B, 128, 16, 16)\n",
        "        e3 = self.conv3_bn(self.conv3(F.leaky_relu(e2, 0.2)))  # (B, 256, 8, 8)\n",
        "        e4 = self.conv4_bn(self.conv4(F.leaky_relu(e3, 0.2)))  # (B, 512, 4, 4)\n",
        "        e5 = self.conv5_bn(self.conv5(F.leaky_relu(e4, 0.2)))  # (B, 512, 2, 2)\n",
        "        e6 = self.conv6_bn(self.conv6(F.leaky_relu(e5, 0.2)))  # (B, 512, 1, 1)\n",
        "\n",
        "        # e7 = self.conv7_bn(self.conv7(F.leaky_relu(e6, 0.2)))\n",
        "        # e8 = self.conv8(F.leaky_relu(e7, 0.2))\n",
        "        # e8 = self.conv8_bn(self.conv8(F.leaky_relu(e7, 0.2)))\n",
        "        # d1 = F.dropout(self.deconv1_bn(self.deconv1(F.relu(e8))), 0.5, training=True)\n",
        "        # d1 = torch.cat([d1, e7], 1)\n",
        "        # d2 = F.dropout(self.deconv2_bn(self.deconv2(F.relu(d1))), 0.5, training=True)\n",
        "        # d2 = torch.cat([d2, e6], 1)\n",
        "\n",
        "        d3 = F.dropout(self.deconv3_bn(self.deconv3(F.relu(e6))), 0.5, training=True)  # (B, 512, 2, 2)\n",
        "        d3 = torch.cat([d3, e5], 1)  # (B, 1024, 2, 2)\n",
        "        d4 = self.deconv4_bn(self.deconv4(F.relu(d3)))  # (B, 512, 4, 4)\n",
        "\n",
        "        # d4 = F.dropout(self.deconv4_bn(self.deconv4(F.relu(d3))), 0.5)\n",
        "\n",
        "        d4 = torch.cat([d4, e4], 1)  # (B, 1024, 4, 4)\n",
        "        d5 = self.deconv5_bn(self.deconv5(F.relu(d4)))  # (B, 256, 8, 8)\n",
        "        d5 = torch.cat([d5, e3], 1)  # (B, 512, 8, 8)\n",
        "        d6 = self.deconv6_bn(self.deconv6(F.relu(d5)))  # (B, 128, 16, 16)\n",
        "        d6 = torch.cat([d6, e2], 1)  # (B, 256, 16, 16)\n",
        "        d7 = self.deconv7_bn(self.deconv7(F.relu(d6)))  # (B, 64, 32, 32)\n",
        "        d7 = torch.cat([d7, e1], 1)  # (B, 128, 32, 32)\n",
        "        d8 = self.deconv8(F.relu(d7))  # (B, 3, 64, 64)\n",
        "        o = F.tanh(d8)  # (B, 3, 64, 64)\n",
        "\n",
        "        return o  # (B, 3, 64, 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchGANDiscriminator(nn.Module):\n",
        "    def __init__(self, d=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(6, d, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(d * 2)\n",
        "        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(d * 4)\n",
        "        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 1, 1)\n",
        "        self.conv4_bn = nn.BatchNorm2d(d * 8)\n",
        "        self.conv5 = nn.Conv2d(d * 8, 1, 4, 1, 1)\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # input: (B, 3, 64, 64)\n",
        "        # label: (B, 3, 64, 64)\n",
        "\n",
        "        x = torch.cat([input, label], 1)  # (B, 6, 64, 64)\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)  # (B, 64, 32, 32)\n",
        "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)  # (B, 128, 16, 16)\n",
        "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)  # (B, 256, 8, 8)\n",
        "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)  # (B, 512, 7, 7)\n",
        "        x = F.sigmoid(self.conv5(x))  # (B, 1, 6, 6)\n",
        "\n",
        "        return x  # (B, 1, 6, 6)"
      ],
      "metadata": {
        "id": "BQjCkIvu1Wmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CycleGAN\n",
        "\n",
        "Ref: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/CycleGAN/generator_model.py"
      ],
      "metadata": {
        "id": "peD2g7Ci0yqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
        "            if down\n",
        "            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
        "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class CycleGANGenerator(nn.Module):\n",
        "    def __init__(self, img_channels, num_features = 64, num_residuals=9):\n",
        "        super().__init__()\n",
        "\n",
        "        # input: (B, 3, 64, 64)\n",
        "\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, num_features, kernel_size=7, \n",
        "                      stride=1, padding=3, padding_mode=\"reflect\"),\n",
        "            nn.InstanceNorm2d(num_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )  # (B, 64, 64, 64)\n",
        "\n",
        "        self.down_blocks = nn.ModuleList(\n",
        "            [\n",
        "                ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),  # (B, 128, 32, 32)\n",
        "                ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),  # (B, 256, 16, 16)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(num_features*4) for _ in range(num_residuals)]  # (B, 256, 16, 16) âˆ€ Residuals\n",
        "        )\n",
        "\n",
        "        self.up_blocks = nn.ModuleList(\n",
        "            [\n",
        "                ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, \n",
        "                          stride=2, padding=1, output_padding=1),  # (B, 128, 32, 32)\n",
        "                ConvBlock(num_features*2, num_features*1, down=False, kernel_size=3, \n",
        "                          stride=2, padding=1, output_padding=1),  # (B, 64, 64, 64)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7, \n",
        "                              stride=1, padding=3, padding_mode=\"reflect\")  # (B, 3, 64, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input: (B, 3, 64, 64)\n",
        "\n",
        "        x = self.initial(x)  # (B, 64, 64, 64)\n",
        "        \n",
        "        for layer in self.down_blocks:\n",
        "            x = layer(x)  # (B, 256, 16, 16)\n",
        "\n",
        "        x = self.res_blocks(x)  # (B, 256, 16, 16)\n",
        "\n",
        "        for layer in self.up_blocks:\n",
        "            x = layer(x)  # (B, 64, 64, 64)\n",
        "\n",
        "        return torch.tanh(self.last(x))  # (B, 3, 64, 64)"
      ],
      "metadata": {
        "id": "HIy2mhb30yLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGANDiscriminatorBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=True, padding_mode=\"reflect\"),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class CycleGANDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "\n",
        "        # input: (B, 3, 64, 64)\n",
        "\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                features[0],\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                padding_mode=\"reflect\",\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )  # (B, 64, 32, 32)\n",
        "\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            # 0: (B, 128, 16, 16)\n",
        "            # 1: (B, 256, 8, 8)\n",
        "            # 2: (B, 512, 7, 7)\n",
        "\n",
        "            layers.append(CycleGANDiscriminatorBlock(in_channels, feature, stride=1 if feature==features[-1] else 2))\n",
        "            in_channels = feature\n",
        "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))  # (B, 1, 6, 6)\n",
        "\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input: (B, 3, 64, 64)\n",
        "\n",
        "        x = self.initial(x)  # (B, 64, 32, 32)\n",
        "        return torch.sigmoid(self.model(x))  # (B, 1, 6, 6)"
      ],
      "metadata": {
        "id": "2aJ44EFY3B_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "Is0uMzJHDY1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Hparams:\n",
        "\n",
        "    # Batch size during training.\n",
        "    batch_size = 128\n",
        "\n",
        "    # Spatial size of training images. All images will be resized to this\n",
        "    # size using a transformer.\n",
        "    image_size = 64\n",
        "\n",
        "    # Number of channels in the training images. For color images this is 3.\n",
        "    nc = 3\n",
        "\n",
        "    # Size of z latent vector (i.e. size of generator input).\n",
        "    nz = 100\n",
        "\n",
        "    # DCGANGenerator size of feature maps.\n",
        "    ngf = 64\n",
        "\n",
        "    # DCGANDiscriminator size of feature maps.\n",
        "    ndf = 64\n",
        "\n",
        "    # Number of training epochs.\n",
        "    num_epochs = 90\n",
        "\n",
        "    # Learning rate for optimizers.\n",
        "    lr = 1e-5\n",
        "\n",
        "    # Beta1 hyperparam for Adam optimizers.\n",
        "    beta1 = 0.5\n",
        "\n",
        "    # Beta2 hyperparam for Adam optimizers.\n",
        "    beta2 = 0.999\n",
        "\n",
        "    # PatchGAN size of feature maps.\n",
        "    d = 64\n",
        "\n",
        "    # PatchGAN L1 aux loss lambda for reducing blurryness. \n",
        "    l1_lambda = 100\n",
        "\n",
        "    # CycleGAN lambda cycle.\n",
        "    lambda_cycle = 10\n",
        "\n",
        "    # CycleGAN identity lambda.\n",
        "    lambda_identity = 0\n",
        "\n",
        "    # CycleGAN number of residual blocks.\n",
        "    num_residuals = 9"
      ],
      "metadata": {
        "id": "0lFyeD5UIwxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = Hparams()"
      ],
      "metadata": {
        "id": "1BGTnes4IzLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXGNa55yGirj"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N97o_bXfG5hV"
      },
      "source": [
        "## Get Models Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_type):\n",
        "    assert model_type in [\"dcgan\", \"patchgan\", \"cyclegan\"], f\"{model_type} is not a valid model.\"\n",
        "\n",
        "    generators, discriminators = [], []\n",
        "\n",
        "    if model_type == \"dcgan\" or model_type == \"patchgan\":\n",
        "        # Create the generator.\n",
        "        if model_type == \"dcgan\":\n",
        "          netG = DCGANGenerator(h.nc, h.ngf).to(c.device)\n",
        "        else:\n",
        "          netG = PatchGANGenerator(h.d).to(c.device)\n",
        "\n",
        "        # Handle multi-gpu if desired.\n",
        "        if (c.device.type == 'cuda') and (c.ngpu > 1):\n",
        "            netG = nn.DataParallel(netG, list(range(c.ngpu)))\n",
        "\n",
        "        # Apply the weights_init function to randomly initialize all weights\n",
        "        #  to mean=0, stdev=0.02.\n",
        "        netG.apply(weights_init)\n",
        "\n",
        "        # Create the Discriminator.\n",
        "        if model_type == \"dcgan\":\n",
        "          netD = DCGANDiscriminator(h.nc, h.ngf).to(c.device)\n",
        "        else:\n",
        "          netD = PatchGANDiscriminator(h.d).to(c.device)\n",
        "\n",
        "        # Handle multi-gpu if desired.\n",
        "        if (c.device.type == 'cuda') and (c.ngpu > 1):\n",
        "            netD = nn.DataParallel(netD, list(range(c.ngpu)))\n",
        "\n",
        "        # Apply the weights_init function to randomly initialize all weights\n",
        "        #  to mean=0, stdev=0.2.\n",
        "        netD.apply(weights_init)\n",
        "\n",
        "        generators.append(netG)\n",
        "        discriminators.append(netD)\n",
        "\n",
        "    elif model_type == \"cyclegan\":\n",
        "        # Create the Generators.\n",
        "        gen_C = CycleGANGenerator(img_channels=h.nc, num_residuals=h.num_residuals).to(c.device)\n",
        "        gen_H = CycleGANGenerator(img_channels=h.nc, num_residuals=h.num_residuals).to(c.device)\n",
        "\n",
        "        # Apply the weights_init function to randomly initialize all weights\n",
        "        #  to mean=0, stdev=0.2.\n",
        "        # gen_C.apply(weights_init)\n",
        "        # gen_H.apply(weights_init)\n",
        "\n",
        "        # Create the Discriminators\n",
        "        disc_C = CycleGANDiscriminator(in_channels=h.nc).to(c.device)\n",
        "        disc_H = CycleGANDiscriminator(in_channels=h.nc).to(c.device)\n",
        "\n",
        "        # Apply the weights_init function to randomly initialize all weights\n",
        "        #  to mean=0, stdev=0.2.\n",
        "        # disc_C.apply(weights_init)\n",
        "        # disc_H.apply(weights_init)\n",
        "\n",
        "        generators.append(gen_C)\n",
        "        generators.append(gen_H)\n",
        "        discriminators.append(disc_C)\n",
        "        discriminators.append(disc_H)\n",
        "\n",
        "    return generators, discriminators"
      ],
      "metadata": {
        "id": "mamLfKccJ6CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3sawVObGyvh"
      },
      "source": [
        "## Get Criterions, Optimizers, and Scalers Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "128zDdGFGflT"
      },
      "source": [
        "def get_criterions(model_type, generators, discriminators):\n",
        "    assert model_type in [\"dcgan\", \"patchgan\", \"cyclegan\"], f\"{model_type} is not a valid model.\"\n",
        "\n",
        "    criterions = []\n",
        "\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    if model_type == \"dcgan\":\n",
        "        netG, netD = generators[0], discriminators[0]\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizerG = optim.Adam(netG.parameters(), lr=h.lr, betas=(h.beta1, h.beta2))\n",
        "        optimizerD = optim.Adam(netD.parameters(), lr=h.lr, betas=(h.beta1, h.beta2))\n",
        "\n",
        "        criterions.append(criterion)\n",
        "\n",
        "    elif model_type == \"patchgan\":\n",
        "        netG, netD = generators[0], discriminators[0]\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        l1_loss = nn.L1Loss()\n",
        "        optimizerG = optim.Adam(netG.parameters(), lr=h.lr, betas=(h.beta1, h.beta2))\n",
        "        optimizerD = optim.Adam(netD.parameters(), lr=h.lr, betas=(h.beta1, h.beta2))\n",
        "\n",
        "        criterions.append(criterion)\n",
        "        criterions.append(l1_loss)\n",
        "\n",
        "    elif model_type == \"cyclegan\":\n",
        "        gen_C, gen_H = generators\n",
        "        disc_C, disc_H = discriminators\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        l1_loss = nn.L1Loss()\n",
        "        optimizerG = optim.Adam(\n",
        "            list(gen_C.parameters()) + list(gen_H.parameters()),\n",
        "            lr=h.lr,\n",
        "            betas=(h.beta1, h.beta2),\n",
        "        )\n",
        "        optimizerD = optim.Adam(\n",
        "            list(disc_H.parameters()) + list(disc_C.parameters()),\n",
        "            lr=h.lr,\n",
        "            betas=(h.beta1, h.beta2),\n",
        "        )\n",
        "\n",
        "        criterions.append(criterion)\n",
        "        criterions.append(l1_loss)\n",
        "\n",
        "    return criterions, optimizerG, optimizerD, g_scaler, d_scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAubVjLCHF7v"
      },
      "source": [
        "## Create the Models, Criterions, Optimizers, Scalers, and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WezRPTZlG1rG"
      },
      "source": [
        "# Model type: [\"dcgan\", \"patchgan\", \"cyclegan\"].\n",
        "model_type = \"cyclegan\"\n",
        "\n",
        "# Get models.\n",
        "generators, discriminators = get_model(model_type)\n",
        "\n",
        "# Get the criterions, optimizers, and scalers.\n",
        "criterions, optimizerG, optimizerD, g_scaler, d_scaler = get_criterions(model_type, generators, discriminators)\n",
        "\n",
        "# Get the dataset and loaders. \n",
        "train_cat_celeba_dataset = AFCatCelebADataset(path_c=train_cat_path, path_h=celeba_path, \n",
        "                               image_size=(h.image_size, h.image_size, h.nc),\n",
        "                               path_c_1=val_cat_path,\n",
        "                               path_c_fferlito=fferlito_path,\n",
        "                               path_c_spandan=spandan_path,\n",
        "                               path_c_waifuai=waifuai_path)\n",
        "\n",
        "train_cat_celeba_loader = DataLoader(train_cat_celeba_dataset, \n",
        "                           batch_size=h.batch_size,\n",
        "                           shuffle=False,\n",
        "                           num_workers=c.workers,\n",
        "                           drop_last=True)\n",
        "\n",
        "fixed_images_dataset = FixedCelebADataset(path=celeba_path, \n",
        "                                         image_size=(h.image_size, h.image_size, h.nc),\n",
        "                                         num_images=c.num_images,\n",
        "                                         total_images=c.total_images)\n",
        "\n",
        "fixed_images_loader = DataLoader(fixed_images_dataset,\n",
        "                                batch_size=c.num_images,\n",
        "                                shuffle=False,\n",
        "                                num_workers=c.workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi3WPcAEHIdg"
      },
      "source": [
        "## Pick & Run Training Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgWw6qwU359L",
        "outputId": "c87d6886-154a-4bc6-ad9b-94830cd8522a"
      },
      "source": [
        "# Checking our GPU.\n",
        "import torch\n",
        "import platform \n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "else:\n",
        "  print(\"\\n[INFO] GPU not found. Using CPU: {}\\n\".format(platform.processor()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla P100-PCIE-16GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_qCH4VzML1h"
      },
      "source": [
        "# Call this if the run never finished.\n",
        "# run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prime the Training Script"
      ],
      "metadata": {
        "id": "kRFqzLJin2QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Method:\n",
        "#     - Manual evaluation with no rating âœ“\n",
        "\n",
        "# Saving Methods: \n",
        "#     - fixed images âœ“\n",
        "#     - systematic saving âœ“\n",
        "#     - last epoch saving âœ“\n",
        "#     - best val_loss saving\n",
        "\n",
        "os.makedirs('gifs/', exist_ok=True)\n",
        "\n",
        "run = wandb.init(project=c.project_name, name=f\"{c.project_run_name}\")\n",
        "\n",
        "d_ckpt_dir, g_ckpt_dir = return_ckpt_dir(\"D\"), return_ckpt_dir(\"G\")\n",
        "img_list, data = [], []\n",
        "\n",
        "# Wandb tables for fixed image visualization.\n",
        "my_table = wandb.Table(columns=[\"Epoch\", \n",
        "                                \"I-th Iteration\", \n",
        "                                \"Fixed Images\"])\n",
        "for fixed_images in fixed_images_loader:\n",
        "  img_list.append(vutils.make_grid(fixed_images[\"images\"], padding=2, normalize=True))\n",
        "  my_table.add_data(0, 0, \n",
        "                    wandb.Image(Image.fromarray((img_list[-1].permute(1, 2, 0).numpy()*255).astype(np.uint8))))\n",
        "\n",
        "# Watch the distribution of gradients and weights for generators and discriminators.\n",
        "if model_type == \"dcgan\" or model_type == \"patchgan\":\n",
        "    netG, netD = generators[0], discriminators[0]\n",
        "\n",
        "    run.watch(netG, log=\"all\", log_freq=10, idx=0)\n",
        "    run.watch(netD, log=\"all\", log_freq=10, idx=1)\n",
        "elif model_type == \"cyclegan\":\n",
        "    gen_H, gen_C = generators\n",
        "    disc_H, disc_C = discriminators\n",
        "        \n",
        "    run.watch(gen_H, log=\"all\", log_freq=10, idx=0)\n",
        "    run.watch(gen_C, log=\"all\", log_freq=10, idx=1)\n",
        "    run.watch(disc_H, log=\"all\", log_freq=10, idx=2)\n",
        "    run.watch(disc_C, log=\"all\", log_freq=10, idx=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "advxjiMKhkhS",
        "outputId": "728a23a5-b919-4a61-b5ce-4334b4e2dc3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/vincenttu/DCGAN_TL2021/runs/20x64hzm\" target=\"_blank\">CycleGAN</a></strong> to <a href=\"https://wandb.ai/vincenttu/DCGAN_TL2021\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CycleGAN Training Script"
      ],
      "metadata": {
        "id": "WBVNndhakslJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training time per epoch for b_size of 128 with 39 batches: 2.5 mins/epoch\n",
        "# Number of batches of size 128: 406 â˜ ï¸\n",
        "# Number of cat images with the added data: 52114 â˜ ï¸"
      ],
      "metadata": {
        "id": "Y31FuwKMdlS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRJ0f7w9HKo7"
      },
      "source": [
        "# Checker.\n",
        "if model_type == \"dcgan\" or model_type == \"patchgan\":\n",
        "    assert False, f\"{model_type} model_type should be 'cyclegan'.\"\n",
        "elif model_type == \"cyclegan\":\n",
        "    gen_C, gen_H = generators\n",
        "    disc_C, disc_H = discriminators\n",
        "    criterion, l1_loss = criterions\n",
        "\n",
        "for epoch in range(1, h.num_epochs + 1):\n",
        "    train_D_loss, train_G_loss = [], []\n",
        "\n",
        "    print('Epoch: {:02d}/{:02d}'.format(epoch, h.num_epochs))\n",
        "    print(\"TRAIN\")\n",
        "\n",
        "    loop = tqdm(enumerate(train_cat_celeba_loader))\n",
        "    for i, human_cat_data in loop:\n",
        "        human_data = human_cat_data[\"human_img\"].to(c.device)\n",
        "        cat_data = human_cat_data[\"cat_img\"].to(c.device)\n",
        "\n",
        "        # Train Discriminators H and C.\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake_human = gen_H(cat_data)\n",
        "            D_H_real = disc_H(human_data)\n",
        "            D_H_fake = disc_H(fake_human.detach())\n",
        "            D_H_real_loss = criterion(D_H_real, torch.ones_like(D_H_real))\n",
        "            D_H_fake_loss = criterion(D_H_fake, torch.zeros_like(D_H_fake))\n",
        "            D_H_loss = D_H_real_loss + D_H_fake_loss\n",
        "\n",
        "            fake_cat = gen_C(human_data)\n",
        "            D_C_real = disc_C(cat_data)\n",
        "            D_C_fake = disc_C(fake_cat.detach())\n",
        "            D_C_real_loss = criterion(D_C_real, torch.ones_like(D_C_real))\n",
        "            D_C_fake_loss = criterion(D_C_fake, torch.zeros_like(D_C_fake))\n",
        "            D_C_loss = D_C_real_loss + D_C_fake_loss\n",
        "\n",
        "            # Combine discriminator losses.\n",
        "            errD = (D_H_loss + D_C_loss)/2\n",
        "\n",
        "        train_D_loss.append(errD.item())\n",
        "\n",
        "        optimizerD.zero_grad()\n",
        "        d_scaler.scale(errD).backward()\n",
        "        d_scaler.step(optimizerD)\n",
        "        d_scaler.update()\n",
        "\n",
        "        # Train Generators H and C.\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # Adversarial loss for both generators.\n",
        "            D_H_fake = disc_H(fake_human)\n",
        "            D_C_fake = disc_C(fake_cat)\n",
        "            loss_G_H = criterion(D_H_fake, torch.ones_like(D_H_fake))\n",
        "            loss_G_C = criterion(D_C_fake, torch.ones_like(D_C_fake))\n",
        "\n",
        "            # Cycle consistency loss.\n",
        "            cycle_cat = gen_C(fake_human)\n",
        "            cycle_human = gen_H(fake_cat)\n",
        "            cycle_cat_loss = l1_loss(cat_data, cycle_cat)\n",
        "            cycle_human_loss = l1_loss(human_data, cycle_human)\n",
        "\n",
        "            # # Identity loss (remove these for efficiency if you set lambda_identity=0).\n",
        "            # identity_cat = gen_C(cat_data)\n",
        "            # identity_human = gen_H(human_data)\n",
        "            # identity_cat_loss = l1_loss(cat_data, identity_cat)\n",
        "            # identity_human_loss = l1_loss(human_data, identity_human)\n",
        "\n",
        "            # Combine generator losses.\n",
        "            errG = (\n",
        "                loss_G_C\n",
        "                + loss_G_H\n",
        "                + cycle_cat_loss * h.lambda_cycle\n",
        "                + cycle_human_loss * h.lambda_cycle\n",
        "                # + identity_human_loss * h.lambda_identity\n",
        "                # + identity_cat_loss * h.lambda_identity\n",
        "            )\n",
        "\n",
        "        train_G_loss.append(errG.item())\n",
        "\n",
        "        optimizerG.zero_grad()\n",
        "        g_scaler.scale(errG).backward()\n",
        "        g_scaler.step(optimizerG)\n",
        "        g_scaler.update()\n",
        "\n",
        "        loop.set_description('D_loss: {:.5f} | G_loss: {:.5f}'.format(errD.item(), errG.item()))\n",
        "        loop.set_postfix(D_loss_mean=np.mean(train_D_loss), G_loss_mean=np.mean(train_G_loss))\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on a fixed image.\n",
        "        if (i % 40 == 0) or (epoch % 10 == 0) and (i == len(train_cat_celeba_loader)-1):\n",
        "            with torch.no_grad():\n",
        "                for fixed_images in fixed_images_loader:\n",
        "                    fake = gen_C(fixed_images[\"images\"].to(c.device)).detach().cpu()\n",
        "                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "            data.append([epoch, i])\n",
        "            my_table.add_data(epoch, i, \n",
        "                              wandb.Image(Image.fromarray((img_list[-1]\n",
        "                                                          .permute(1, 2, 0)\n",
        "                                                          .numpy()*255)\n",
        "                                                          .astype(np.uint8))))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(gen_C.state_dict(), g_ckpt_dir)\n",
        "        torch.save(disc_C.state_dict(), d_ckpt_dir)\n",
        "\n",
        "        artifact = wandb.Artifact(c.model_name, type='model')\n",
        "        artifact.add_file(g_ckpt_dir, name=f\"G_C_epoch{epoch}.pth\")\n",
        "        artifact.add_file(d_ckpt_dir, name=f\"D_C_epoch{epoch}.pth\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        torch.save(gen_H.state_dict(), g_ckpt_dir)\n",
        "        torch.save(disc_H.state_dict(), d_ckpt_dir)\n",
        "\n",
        "        artifact = wandb.Artifact(c.model_name, type='model')\n",
        "        artifact.add_file(g_ckpt_dir, name=f\"G_H_epoch{epoch}.pth\")\n",
        "        artifact.add_file(d_ckpt_dir, name=f\"D_H_epoch{epoch}.pth\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "    wandb.log({\"epoch\": epoch, \n",
        "              \"G_loss\": np.mean(train_G_loss),\n",
        "              \"D_loss\": np.mean(train_D_loss), \n",
        "              })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PatchGAN Training Script"
      ],
      "metadata": {
        "id": "PyLFq9vY48tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checker.\n",
        "if model_type == \"dcgan\" or model_type == \"cyclegan\":\n",
        "    assert False, f\"{model_type} model_type should be 'patchgan'.\"\n",
        "elif model_type == \"patchgan\":\n",
        "    netG = generators[0]\n",
        "    netD = discriminators[0]\n",
        "    criterion, l1_loss = criterions\n",
        "\n",
        "for epoch in range(1, h.num_epochs + 1):\n",
        "    train_D_loss, train_G_loss = [], []\n",
        "\n",
        "    print('Epoch: {:02d}/{:02d}'.format(epoch, h.num_epochs))\n",
        "    print(\"TRAIN\")\n",
        "\n",
        "    loop = tqdm(enumerate(train_cat_celeba_loader))\n",
        "    for i, human_cat_data in loop:\n",
        "        human_data = human_cat_data[\"human_img\"].to(c.device)\n",
        "        cat_data = human_cat_data[\"cat_img\"].to(c.device)\n",
        "\n",
        "        # Train discriminator.\n",
        "        with torch.cuda.amp.autocast():\n",
        "            y_fake = netG(human_data)\n",
        "            D_real = netD(human_data, cat_data)\n",
        "            D_real_loss = criterion(D_real, torch.ones_like(D_real))\n",
        "            D_fake = netD(human_data, y_fake.detach())\n",
        "            D_fake_loss = criterion(D_fake, torch.zeros_like(D_fake))\n",
        "            errD = (D_real_loss + D_fake_loss) / 2\n",
        "\n",
        "        train_D_loss.append(errD.item())\n",
        "\n",
        "        netD.zero_grad()\n",
        "        d_scaler.scale(errD).backward()\n",
        "        d_scaler.step(optimizerD)\n",
        "        d_scaler.update()\n",
        "\n",
        "        # Train generator.\n",
        "        with torch.cuda.amp.autocast():\n",
        "            D_fake = netD(human_data, y_fake)\n",
        "            G_fake_loss = criterion(D_fake, torch.ones_like(D_fake))\n",
        "            L1 = l1_loss(y_fake, cat_data) * h.l1_lambda\n",
        "            errG = G_fake_loss + L1\n",
        "\n",
        "        train_G_loss.append(errG.item())\n",
        "\n",
        "        optimizerG.zero_grad()\n",
        "        g_scaler.scale(errG).backward()\n",
        "        g_scaler.step(optimizerG)\n",
        "        g_scaler.update()\n",
        "\n",
        "        loop.set_description('D_loss: {:.5f} | G_loss: {:.5f}'.format(errD.item(), errG.item()))\n",
        "        loop.set_postfix(D_loss_mean=np.mean(train_D_loss), G_loss_mean=np.mean(train_G_loss))\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on a fixed image.\n",
        "        if (i % 40 == 0) or (epoch % 10 == 0) and (i == len(train_cat_celeba_loader)-1):\n",
        "            with torch.no_grad():\n",
        "                for fixed_images in fixed_images_loader:\n",
        "                    fake = netG(fixed_images[\"images\"].to(c.device)).detach().cpu()\n",
        "                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "            data.append([epoch, i])\n",
        "            my_table.add_data(epoch, i, \n",
        "                              wandb.Image(Image.fromarray((img_list[-1]\n",
        "                                                          .permute(1, 2, 0)\n",
        "                                                          .numpy()*255)\n",
        "                                                          .astype(np.uint8))))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(netG.state_dict(), g_ckpt_dir)\n",
        "        torch.save(netD.state_dict(), d_ckpt_dir)\n",
        "\n",
        "        artifact = wandb.Artifact(c.model_name, type='model')\n",
        "        artifact.add_file(g_ckpt_dir, name=f\"G_epoch{epoch}.pth\")\n",
        "        artifact.add_file(d_ckpt_dir, name=f\"D_epoch{epoch}.pth\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "    wandb.log({\"epoch\": epoch, \n",
        "              \"G_loss\": np.mean(train_G_loss),\n",
        "              \"D_loss\": np.mean(train_D_loss), \n",
        "              })"
      ],
      "metadata": {
        "id": "QwjbtMMD48Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DCGAN Training Script"
      ],
      "metadata": {
        "id": "XmSvGKy5O5rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checker.\n",
        "if model_type == \"patchgan\" or model_type == \"cyclegan\":\n",
        "    assert False, f\"{model_type} model_type should be 'dcgan'.\"\n",
        "elif model_type == \"dcgan\":\n",
        "    netG = generators[0]\n",
        "    netD = discriminators[0]\n",
        "    criterion = criterions[0]\n",
        "\n",
        "for epoch in range(1, h.num_epochs + 1):\n",
        "    train_D_loss, train_G_loss = [], []\n",
        "\n",
        "    print('Epoch: {:02d}/{:02d}'.format(epoch, h.num_epochs))\n",
        "    print(\"TRAIN\")\n",
        "\n",
        "    loop = tqdm(enumerate(train_cat_celeba_loader))\n",
        "    for i, human_cat_data in loop:\n",
        "        human_data = human_cat_data[\"human_img\"].to(c.device)\n",
        "        cat_data = human_cat_data[\"cat_img\"].to(c.device)\n",
        "\n",
        "        # Train discriminator.\n",
        "        netD.zero_grad()\n",
        "        real_cpu = cat_data\n",
        "        b_size = real_cpu.size(0)\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        errD_real = criterion(output, torch.ones_like(output))\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        noise = human_data\n",
        "        fake = netG(noise)\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        errD_fake = criterion(output, torch.zeros_like(output))\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        train_D_loss.append(errD.item())\n",
        "\n",
        "        # Train generator.\n",
        "        netG.zero_grad()\n",
        "        output = netD(fake).view(-1)\n",
        "        errG = criterion(output, torch.ones_like(output))\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        train_G_loss.append(errG.item())\n",
        "\n",
        "        loop.set_description('D_loss: {:.5f} | G_loss: {:.5f}'.format(errD.item(), errG.item()))\n",
        "        loop.set_postfix(D_loss_mean=np.mean(train_D_loss), G_loss_mean=np.mean(train_G_loss))\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on a fixed image.\n",
        "        if (i % 40 == 0) or (epoch % 10 == 0) and (i == len(train_cat_celeba_loader)-1):\n",
        "            with torch.no_grad():\n",
        "                for fixed_images in fixed_images_loader:\n",
        "                    fake = netG(fixed_images[\"images\"].to(c.device)).detach().cpu()\n",
        "                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "            data.append([epoch, i])\n",
        "            my_table.add_data(epoch, i, \n",
        "                              wandb.Image(Image.fromarray((img_list[-1]\n",
        "                                                          .permute(1, 2, 0)\n",
        "                                                          .numpy()*255)\n",
        "                                                          .astype(np.uint8))))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(netG.state_dict(), g_ckpt_dir)\n",
        "        torch.save(netD.state_dict(), d_ckpt_dir)\n",
        "\n",
        "        artifact = wandb.Artifact(c.model_name, type='model')\n",
        "        artifact.add_file(g_ckpt_dir, name=f\"G_epoch{epoch}.pth\")\n",
        "        artifact.add_file(d_ckpt_dir, name=f\"D_epoch{epoch}.pth\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "    wandb.log({\"epoch\": epoch, \n",
        "              \"G_loss\": np.mean(train_G_loss),\n",
        "              \"D_loss\": np.mean(train_D_loss), \n",
        "              })"
      ],
      "metadata": {
        "id": "cRaCG6eRO5VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log and Garbage Collect"
      ],
      "metadata": {
        "id": "TWiD0OtblxYf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "4597e93b08e64d899380e68f6d1420a2",
            "fe6f4ac069514770941f8833e92dd4c0",
            "a5f5e84d21ca40868e40ac7c42b31f77",
            "099374e302c6494d88830736fd0a104a",
            "246c19413a8248efa21c2267c4493dfe",
            "ea047b5de5b94db4a40d73e05ab1d7f2",
            "cfba2fde04024303886c44d5c6503753",
            "03bf12ce12e24cbf9d4fb1e49cc41d8e"
          ]
        },
        "id": "RHH7ARDq6DJu",
        "outputId": "ccc15b29-b4af-4348-a399-4117956c7418"
      },
      "source": [
        "# Run this block if you ended the training script prematurely.\n",
        "\n",
        "img_frames = [(frame*255).permute(1, 2, 0).numpy().astype(\"uint8\") for frame in img_list]\n",
        "if len(img_frames): imageio.mimsave(f'gifs/ani.gif', img_frames)\n",
        "\n",
        "run.log({f\"Fixed Images Inspection\": my_table})\n",
        "run.log({'Fixed Images Animation': \n",
        "         [wandb.Image(f'gifs/ani.gif')]})\n",
        "\n",
        "if model_type == \"dcgan\" or model_type == \"patchgan\":\n",
        "    del netG, netD\n",
        "elif model_type == \"cyclegan\":\n",
        "    del gen_C, gen_H, disc_C, disc_H \n",
        "del optimizerD, optimizerG\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 565... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4597e93b08e64d899380e68f6d1420a2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 1.23MB of 1.23MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.99992258821â€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">CycleGAN</strong>: <a href=\"https://wandb.ai/vincenttu/DCGAN_TL2021/runs/20x64hzm\" target=\"_blank\">https://wandb.ai/vincenttu/DCGAN_TL2021/runs/20x64hzm</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211218_071252-20x64hzm/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archive"
      ],
      "metadata": {
        "id": "RK5BJK4HE2_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archived Preprocessing"
      ],
      "metadata": {
        "id": "CvjWHgC7E4va"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XgaozbBF1Es"
      },
      "source": [
        "## Animal-Faces Dataset Preprocessing (archived)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEP_y1uqF63e"
      },
      "source": [
        "Reference: https://www.kaggle.com/andrewmvd/animal-faces\n",
        "\n",
        "Dataset Debrief:\n",
        "\n",
        "---\n",
        "\n",
        "- Expected shape of (512, 512, 3)\n",
        "\n",
        "\n",
        "- train/cat: 5153 files jpg same shape (I presume)\n",
        "- train/dog: 4739 files jpg same shape (I presume)\n",
        "- train/wild (foxes): 4738 files jpg same shape (I presume)\n",
        "- val/cat: 500 files jpg same shape (I presume)\n",
        "- val/dog: 500 files jpg same shape (I presume)\n",
        "- val/wild (foxes): 500 files jpg same shape (I presume)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vP6hCJJFZwv"
      },
      "source": [
        "train_cat_path = r\"/content/animal-faces/afhq/train/cat\"\n",
        "train_dog_path = r\"/content/animal-faces/afhq/train/dog\"\n",
        "train_wild_path = r\"/content/animal-faces/afhq/train/wild\"\n",
        "\n",
        "val_cat_path = r\"/content/animal-faces/afhq/val/cat\"\n",
        "val_dog_path = r\"/content/animal-faces/afhq/val/dog\"\n",
        "val_wild_path = r\"/content/animal-faces/afhq/val/wild\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1_SNNvnF_Md"
      },
      "source": [
        "### Checks and Assumptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5ex6Vg3F86p",
        "outputId": "be3363f5-67e6-47d8-9f6e-6c8aa2ccff45"
      },
      "source": [
        "'''\n",
        "\n",
        "# Checking file counts.\n",
        "assert len(list(os.listdir(train_cat_path))) == 5153, \"There aren't 5153 train cat files!\"\n",
        "assert len(list(os.listdir(train_dog_path))) == 4739, \"There aren't 4739 train dog files!\"\n",
        "assert len(list(os.listdir(train_wild_path))) == 4738, \"There aren't 4738 train wild files!\"\n",
        "\n",
        "assert len(list(os.listdir(val_cat_path))) == 500, \"There aren't 500 val cat files!\"\n",
        "assert len(list(os.listdir(val_dog_path))) == 500, \"There aren't 500 val dog files!\"\n",
        "assert len(list(os.listdir(val_wild_path))) == 500, \"There aren't 500 val wild files!\"\n",
        "\n",
        "# Checking the extensions.\n",
        "def check_extensions(path, extension=\"jpg\"):\n",
        "  for idx, img_path in enumerate(glob(os.path.join(path, \"*\"))):\n",
        "    assert extension in img_path, f\"Index {idx} does not have the {extension} extension!\"\n",
        "  \n",
        "for p in [train_cat_path, train_dog_path, train_wild_path, \n",
        "          val_cat_path, val_dog_path, val_wild_path]:\n",
        "  check_extensions(p)\n",
        "\n",
        "# Checking image sizes.\n",
        "def check_img_size(path, expected_img_shape=(512, 512, 3)):\n",
        "  for idx, img_path in enumerate(glob(os.path.join(path, \"*\"))):\n",
        "    assert np.asarray(Image.open(img_path)).shape == expected_img_shape, f\"Index {idx} does not have the expected shape of {expected_img_shape}!\"\n",
        "\n",
        "for p in [train_cat_path, train_dog_path, train_wild_path, \n",
        "          val_cat_path, val_dog_path, val_wild_path]:\n",
        "  check_img_size(p)\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n# Checking file counts.\\nassert len(list(os.listdir(train_cat_path))) == 5153, \"There aren\\'t 5153 train cat files!\"\\nassert len(list(os.listdir(train_dog_path))) == 4739, \"There aren\\'t 4739 train dog files!\"\\nassert len(list(os.listdir(train_wild_path))) == 4738, \"There aren\\'t 4738 train wild files!\"\\n\\nassert len(list(os.listdir(val_cat_path))) == 500, \"There aren\\'t 500 val cat files!\"\\nassert len(list(os.listdir(val_dog_path))) == 500, \"There aren\\'t 500 val dog files!\"\\nassert len(list(os.listdir(val_wild_path))) == 500, \"There aren\\'t 500 val wild files!\"\\n\\n# Checking the extensions.\\ndef check_extensions(path, extension=\"jpg\"):\\n  for idx, img_path in enumerate(glob(os.path.join(path, \"*\"))):\\n    assert extension in img_path, f\"Index {idx} does not have the {extension} extension!\"\\n  \\nfor p in [train_cat_path, train_dog_path, train_wild_path, \\n          val_cat_path, val_dog_path, val_wild_path]:\\n  check_extensions(p)\\n\\n# Checking image sizes.\\ndef check_img_size(path, expected_img_shape=(512, 512, 3)):\\n  for idx, img_path in enumerate(glob(os.path.join(path, \"*\"))):\\n    assert np.asarray(Image.open(img_path)).shape == expected_img_shape, f\"Index {idx} does not have the expected shape of {expected_img_shape}!\"\\n\\nfor p in [train_cat_path, train_dog_path, train_wild_path, \\n          val_cat_path, val_dog_path, val_wild_path]:\\n  check_img_size(p)\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eob3OpIqGCsP"
      },
      "source": [
        "### Building the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKYD4JkGAv0"
      },
      "source": [
        "class AFCatDataset(Dataset):\n",
        "    def __init__(self, path, image_size, mode):\n",
        "        super(AFCatDataset, self).__init__()\n",
        "\n",
        "        self.path = list(glob(os.path.join(path, \"*\")))\n",
        "        self.image_size = image_size\n",
        "        assert mode in ['train', 'valid']\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "          self.transform = A.Compose([\n",
        "            A.Resize(self.image_size[0], self.image_size[1]),\n",
        "            A.CenterCrop(self.image_size[0], self.image_size[1]),\n",
        "            A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ToTensorV2(),\n",
        "          ])\n",
        "        else:\n",
        "          self.transform = A.Compose([\n",
        "            A.Resize(self.image_size[0], self.image_size[1]),\n",
        "            A.CenterCrop(self.image_size[0], self.image_size[1]),\n",
        "            A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ToTensorV2(),\n",
        "          ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "          img = np.asarray(Image.open(self.path[index]))\n",
        "          img = self.transform(image=img)[\"image\"]\n",
        "\n",
        "          return {\"images\": img,\n",
        "                  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-w1xbvPGGra"
      },
      "source": [
        "## CelebA Dataset Preprocessing (archived)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgZbPegRGKVh"
      },
      "source": [
        "Reference: https://www.kaggle.com/zuozhaorui/celeba\n",
        "\n",
        "Dataset Debrief:\n",
        "\n",
        "---\n",
        "\n",
        "- Expected shape of (218, 178, 3)\n",
        "\n",
        "\n",
        "- img_align_celeba/img_align/celeba: ~203k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqN_hgbwGFlN"
      },
      "source": [
        "celeba_path = r\"/content/celeba/img_align_celeba/img_align_celeba\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44v4Wlm2GOiT"
      },
      "source": [
        "### Checks and Assumptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yg0PEg7GLtW",
        "outputId": "4ac85f52-0100-4a4b-df0a-8584a3a3bf79"
      },
      "source": [
        "'''\n",
        "\n",
        "# Checking file counts.\n",
        "assert len(list(os.listdir(celeba_path))) == 202599, \"There aren't 202599 files!\"\n",
        "\n",
        "# Checking the extensions.\n",
        "for p in [celeba_path]:\n",
        "  check_extensions(p)\n",
        "\n",
        "# Checking image sizes.\n",
        "for p in [celeba_path]:\n",
        "  check_img_size(p, expected_img_shape=(218, 178, 3))\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n# Checking file counts.\\nassert len(list(os.listdir(celeba_path))) == 202599, \"There aren\\'t 202599 files!\"\\n\\n# Checking the extensions.\\nfor p in [celeba_path]:\\n  check_extensions(p)\\n\\n# Checking image sizes.\\nfor p in [celeba_path]:\\n  check_img_size(p, expected_img_shape=(218, 178, 3))\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39D5AQgjGP-P"
      },
      "source": [
        "### Building the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S24rDGZ5GNDx"
      },
      "source": [
        "class CelebADataset(Dataset):\n",
        "    def __init__(self, path, image_size, mode):\n",
        "        super(CelebADataset, self).__init__()\n",
        "\n",
        "        self.path = list(glob(os.path.join(path, \"*\")))  # We will slice this dataset so the num images will match the AFCatDataset.\n",
        "        # That means we will set shuffle=True when we create the Dataloader for CelebA.\n",
        "        self.image_size = image_size\n",
        "        assert mode in ['train', 'valid']\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "          self.transform = A.Compose([\n",
        "            # A.HorizontalFlip(p=0.5),\n",
        "            # A.ColorJitter(p=0.2),\n",
        "\n",
        "            A.Resize(self.image_size[0], self.image_size[1]),\n",
        "            A.CenterCrop(self.image_size[0], self.image_size[1]),\n",
        "            A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ToTensorV2(),\n",
        "          ])\n",
        "        else:\n",
        "          self.transform = A.Compose([\n",
        "            A.Resize(self.image_size[0], self.image_size[1]),\n",
        "            A.CenterCrop(self.image_size[0], self.image_size[1]),\n",
        "            A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ToTensorV2(),\n",
        "          ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "          img = np.asarray(Image.open(self.path[index]))\n",
        "          img = self.transform(image=img)[\"image\"]\n",
        "\n",
        "          return {\"images\": img,\n",
        "                  }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}